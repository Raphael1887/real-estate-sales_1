import os
import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ======================
#     FONCTION 1 : LOAD & CLEAN
# ======================

def load_and_clean_data(csv_path):
    """
    Charge le fichier CSV, nettoie les donn√©es :
      - supprime les doublons
      - g√®re les valeurs manquantes
      - v√©rifie les types de donn√©es
    """
    print("üìÇ Chargement des donn√©es...")
    df = pd.read_csv(csv_path, low_memory=False)
    print(f"‚úÖ {len(df):,} lignes charg√©es")

    # Normaliser les noms de colonnes
    df.columns = df.columns.str.lower().str.strip()

    # V√©rification des colonnes attendues
    expected_cols = ["surface", "chambres", "age_bien", "quartier_score", "distance_centre", "prix"]
    missing_cols = [col for col in expected_cols if col not in df.columns]
    if missing_cols:
        raise KeyError(f"Colonnes manquantes : {missing_cols}")

    # Supprimer les doublons
    df = df.drop_duplicates()

    # Gestion des valeurs manquantes
    missing_before = df.isna().sum().sum()
    df = df.dropna(subset=["prix"], how="any")
    df = df.fillna(df.median(numeric_only=True))
    missing_after = df.isna().sum().sum()

    print(f"üßπ Valeurs manquantes avant : {missing_before} ‚Üí apr√®s : {missing_after}")

    # Convertir les types en num√©riques si besoin
    numeric_cols = ["surface", "chambres", "age_bien", "quartier_score", "distance_centre", "prix"]
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    print("‚úÖ Donn√©es nettoy√©es avec succ√®s")
    return df


# ======================
#     FONCTION 2 : EXPLORE DATA
# ======================

def explore_data(df):
    """
    Explore les donn√©es :
      - affiche les statistiques descriptives
      - calcule les corr√©lations
      - identifie les outliers
    """
    print("\nüîç Exploration des donn√©es")

    print("\nüìä Statistiques descriptives :")
    print(df.describe().transpose())

    print("\nüìà Corr√©lations (top 5 avec le prix) :")
    corr = df.corr(numeric_only=True)["prix"].sort_values(ascending=False).head(5)
    print(corr)

    # D√©tection des outliers sur la variable cible
    q1 = df["prix"].quantile(0.25)
    q3 = df["prix"].quantile(0.75)
    iqr = q3 - q1
    outliers = df[(df["prix"] < q1 - 1.5 * iqr) | (df["prix"] > q3 + 1.5 * iqr)]
    print(f"\n‚ö†Ô∏è Outliers d√©tect√©s : {len(outliers):,} lignes ({len(outliers)/len(df)*100:.2f}%)")


# ======================
#     FONCTION 3 : TRAIN MODEL
# ======================

def train_model(df):
    """
    Entra√Æne un mod√®le Random Forest pour pr√©dire le prix.

    -----
    4.1 Comprendre le mod√®le choisi
      ‚Ä¢ Algorithme : Random Forest Regressor
      ‚úì Robuste aux outliers
      ‚úì G√®re bien les relations non-lin√©aires
      ‚úì Peu de pr√©traitement n√©cessaire
      ‚úì Fournit l‚Äôimportance des features
      ‚úì Bonnes performances g√©n√©rales

    -----
    4.2 Hyperparam√®tres choisis :
      ‚Ä¢ n_estimators=100     ‚Üí Nombre d'arbres
      ‚Ä¢ max_depth=20         ‚Üí Profondeur maximale
      ‚Ä¢ min_samples_split=5  ‚Üí √âchantillons min pour split
      ‚Ä¢ random_state=42      ‚Üí Reproductibilit√©
    """
    print("\nü§ñ Entra√Ænement du mod√®le Random Forest...")

    features = ["surface", "chambres", "age_bien", "quartier_score", "distance_centre"]
    X = df[features]
    y = df["prix"]

    # 1. S√©paration features / target
    # 2. Split train/test (80/20)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # 3. Entra√Ænement du mod√®le
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=20,
        min_samples_split=5,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)

    print(f"‚úÖ Mod√®le Random Forest entra√Æn√© avec {len(X_train):,} √©chantillons")

    # 6. Sauvegarde du mod√®le
    model_path = os.path.join(os.path.dirname(__file__), "..", "api", "model", "housing_model.pkl")
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    joblib.dump(model, model_path)
    print(f"üíæ Mod√®le sauvegard√© dans : {model_path}")

    return model, X_test, y_test


# ======================
#     FONCTION 4 : TEST MODEL
# ======================

def test_model_predictions(model, X_test, y_test):
    """
    √âvalue le mod√®le sur l‚Äô√©chantillon de test et affiche les m√©triques.
    """
    print("\nüß™ √âvaluation du mod√®le...")

    # 4. Pr√©dictions sur test set
    y_pred = model.predict(X_test)

    # 5. Calcul des m√©triques
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print(f"üìâ MAE  : {mae:,.2f}")
    print(f"üìâ RMSE : {rmse:,.2f}")
    print(f"üìà R¬≤   : {r2:.3f}")

    # ======================
    #     TESTER DES PR√âDICTIONS MANUELLES
    # ======================
    print("\nüßÆ Tester des pr√©dictions manuelles (5 exemples)")
    print("   Valeur r√©elle vs valeur pr√©dite + erreurs")

    sample_df = X_test.copy()
    sample_df["prix_reel"] = y_test
    sample_df["prix_pred"] = y_pred
    sample_df["erreur_absolue"] = abs(sample_df["prix_reel"] - sample_df["prix_pred"])
    sample_df["erreur_relative_%"] = (sample_df["erreur_absolue"] / sample_df["prix_reel"]) * 100

    print(sample_df[["prix_reel", "prix_pred", "erreur_absolue", "erreur_relative_%"]]
          .head(5)
          .round(2)
          .to_string(index=False))

    return {"MAE": mae, "RMSE": rmse, "R2": r2}


# ======================
#     MAIN PIPELINE
# ======================

def main():
    """Pipeline complet ETL + entra√Ænement mod√®le Random Forest."""
    data_path = os.path.join(os.path.dirname(__file__), "..", "data", "housing_data.csv")

    # √âtape 1 : Extraction & nettoyage
    df = load_and_clean_data(data_path)

    # √âtape 2 : Exploration
    explore_data(df)

    # √âtape 3 : Entra√Ænement du mod√®le
    model, X_test, y_test = train_model(df)

    # √âtape 4 : √âvaluation et test de pr√©dictions
    test_model_predictions(model, X_test, y_test)

    print("\nüéâ Pipeline ETL termin√© avec succ√®s !")


if __name__ == "__main__":
    main()